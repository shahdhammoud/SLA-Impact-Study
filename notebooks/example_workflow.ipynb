{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Workflow: Evaluating Structural Learning Impact\n",
    "\n",
    "This notebook demonstrates the complete workflow for assessing how different structure learning algorithms affect generative model rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Exploration\n",
    "\n",
    "First, let's explore the available datasets and their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.data.loader import DataLoader\n",
    "from src.utils.graph_utils import save_graph_visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loader = DataLoader()\n",
    "datasets = loader.list_available_datasets()\n",
    "print(f\"Available datasets: {datasets}\")\n",
    "\n",
    "if datasets:\n",
    "    dataset_name = datasets[0]\n",
    "    info = loader.get_dataset_info(dataset_name)\n",
    "    print(f\"\\nDataset: {info['name']}\")\n",
    "    print(f\"Samples: {info['n_samples']}\")\n",
    "    print(f\"Features: {info['n_features']}\")\n",
    "    print(f\"Causal edges: {info['n_edges']}\")\n",
    "    print(f\"Feature names: {info['features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Preprocess Data\n",
    "\n",
    "Load and preprocess a dataset, automatically detecting categorical and continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preprocessor import DataPreprocessor\n",
    "\n",
    "# Load raw data\n",
    "data, structure = loader.load_dataset(dataset_name)\n",
    "print(f\"Loaded data shape: {data.shape}\")\n",
    "print(f\"Structure: {structure.number_of_nodes()} nodes, {structure.number_of_edges()} edges\")\n",
    "\n",
    "# Preprocess\n",
    "preprocessor = DataPreprocessor(categorical_threshold=10)\n",
    "data_processed = preprocessor.fit_transform(data)\n",
    "\n",
    "feature_info = preprocessor.get_feature_info()\n",
    "print(f\"\\nCategorical features ({feature_info['n_categorical']}): {feature_info['categorical_features']}\")\n",
    "print(f\"Continuous features ({feature_info['n_continuous']}): {feature_info['continuous_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Generative Models\n",
    "\n",
    "Train multiple generative models on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.gmm_wrapper import GMMWrapper\n",
    "from src.models.bayesian_network import BayesianNetworkWrapper\n",
    "\n",
    "models = {}\n",
    "\n",
    "print(\"Training GMM...\")\n",
    "gmm = GMMWrapper(n_components=5)\n",
    "gmm.fit(data_processed)\n",
    "models['gmm'] = gmm\n",
    "print(\"GMM trained successfully\")\n",
    "\n",
    "print(\"\\nTraining Bayesian Network...\")\n",
    "bn = BayesianNetworkWrapper()\n",
    "bn.fit(data_processed)\n",
    "models['bayesian_network'] = bn\n",
    "print(\"Bayesian Network trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Synthetic Data\n",
    "\n",
    "Generate synthetic samples from trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(data)\n",
    "synthetic_datasets = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Generating {n_samples} samples from {model_name}...\")\n",
    "    synthetic_data = model.sample(n_samples)\n",
    "    \n",
    "    synthetic_data_original = preprocessor.inverse_transform(synthetic_data)\n",
    "    synthetic_datasets[model_name] = synthetic_data_original\n",
    "    \n",
    "    print(f\"Generated shape: {synthetic_data_original.shape}\")\n",
    "    print(f\"Sample:\\n{synthetic_data_original.head()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Learn Causal Structures\n",
    "\n",
    "Apply different structure learning algorithms to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.structure_learning.pc import PCLearner\n",
    "from src.structure_learning.ges import GESLearner\n",
    "from src.utils.graph_utils import compute_graph_metrics\n",
    "\n",
    "learned_structures = {}\n",
    "\n",
    "print(\"Learning structure with PC algorithm...\")\n",
    "pc_learner = PCLearner(alpha=0.05)\n",
    "pc_graph = pc_learner.fit(data_processed)\n",
    "learned_structures['pc'] = pc_graph\n",
    "pc_metrics = compute_graph_metrics(structure, pc_graph)\n",
    "print(f\"PC: {pc_graph.number_of_edges()} edges, SHD={pc_metrics['shd']}, F1={pc_metrics['f1']:.3f}\")\n",
    "\n",
    "print(\"\\nLearning structure with GES algorithm...\")\n",
    "ges_learner = GESLearner()\n",
    "ges_graph = ges_learner.fit(data_processed)\n",
    "learned_structures['ges'] = ges_graph\n",
    "ges_metrics = compute_graph_metrics(structure, ges_graph)\n",
    "print(f\"GES: {ges_graph.number_of_edges()} edges, SHD={ges_metrics['shd']}, F1={ges_metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Models with Different Structures\n",
    "\n",
    "Evaluate each generative model using both ground truth and learned structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.cautabbench_eval import CauTabBenchEvaluator\n",
    "\n",
    "evaluator = CauTabBenchEvaluator(method='fisherz', alpha=0.05)\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for struct_name in ['ground_truth', 'pc', 'ges']:\n",
    "    causal_graph = structure if struct_name == 'ground_truth' else learned_structures[struct_name]\n",
    "    \n",
    "    print(f\"\\nEvaluating with {struct_name} structure:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    struct_results = {}\n",
    "    \n",
    "    for model_name, synthetic_data in synthetic_datasets.items():\n",
    "        real_transformed = preprocessor.transform(data)\n",
    "        synth_transformed = preprocessor.transform(synthetic_data)\n",
    "        \n",
    "        results = evaluator.evaluate(real_transformed, synth_transformed, causal_graph)\n",
    "        struct_results[model_name] = results\n",
    "        \n",
    "        print(f\"{model_name:20s}: Quality={results['quality_score']:.4f}, \"\n",
    "              f\"Agreement={results['agreement_rate']:.4f}\")\n",
    "    \n",
    "    evaluation_results[struct_name] = struct_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare Rankings\n",
    "\n",
    "Compare how model rankings change across different structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.ranking import RankingComparator\n",
    "\n",
    "comparator = RankingComparator()\n",
    "\n",
    "for struct_name, results in evaluation_results.items():\n",
    "    rankings = [(model, res['quality_score']) for model, res in results.items()]\n",
    "    rankings.sort(key=lambda x: x[1], reverse=True)\n",
    "    comparator.add_ranking(struct_name, rankings)\n",
    "    \n",
    "    print(f\"\\n{struct_name} ranking:\")\n",
    "    for i, (model, score) in enumerate(rankings, 1):\n",
    "        print(f\"  {i}. {model:20s}: {score:.4f}\")\n",
    "\n",
    "if len(comparator.comparisons) > 1:\n",
    "    comparison = comparator.compare_rankings('ground_truth')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Ranking Correlation with Ground Truth:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for struct, metrics in comparison['comparisons'].items():\n",
    "        print(f\"\\n{struct}:\")\n",
    "        print(f\"  Kendall's tau: {metrics['kendall_tau']:.3f}\")\n",
    "        print(f\"  Spearman's rho: {metrics['spearman_rho']:.3f}\")\n",
    "        print(f\"  Top model match: {metrics['top_model_match']}\")\n",
    "        print(f\"  Avg positional difference: {metrics['avg_positional_difference']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This workflow demonstrated:\n",
    "1. Data preprocessing with automatic feature type detection\n",
    "2. Training multiple generative models\n",
    "3. Learning causal structures with different algorithms\n",
    "4. Evaluating models using CauTabBench methodology\n",
    "5. Comparing rankings across different structure learning methods\n",
    "\n",
    "Key insights:\n",
    "- Model rankings can change significantly depending on the structure used\n",
    "- Some structure learning algorithms preserve ranking better than others\n",
    "- The choice of structure learning algorithm matters for fair model comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
